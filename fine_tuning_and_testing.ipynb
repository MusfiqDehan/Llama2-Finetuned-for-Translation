{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-14T01:38:36.391559Z","iopub.status.busy":"2024-03-14T01:38:36.391191Z","iopub.status.idle":"2024-03-14T01:38:36.739426Z","shell.execute_reply":"2024-03-14T01:38:36.738679Z","shell.execute_reply.started":"2024-03-14T01:38:36.391529Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# SetUp Directory"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:38:41.528527Z","iopub.status.busy":"2024-03-14T01:38:41.528023Z","iopub.status.idle":"2024-03-14T01:38:41.535517Z","shell.execute_reply":"2024-03-14T01:38:41.534598Z","shell.execute_reply.started":"2024-03-14T01:38:41.528494Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["%pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:38:45.288761Z","iopub.status.busy":"2024-03-14T01:38:45.288423Z","iopub.status.idle":"2024-03-14T01:38:45.294711Z","shell.execute_reply":"2024-03-14T01:38:45.293886Z","shell.execute_reply.started":"2024-03-14T01:38:45.288736Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle\n"]}],"source":["%cd .."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:38:48.339892Z","iopub.status.busy":"2024-03-14T01:38:48.339531Z","iopub.status.idle":"2024-03-14T01:38:49.279600Z","shell.execute_reply":"2024-03-14T01:38:49.278570Z","shell.execute_reply.started":"2024-03-14T01:38:48.339863Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34minput\u001b[0m/  \u001b[01;34mlib\u001b[0m/  \u001b[01;34mworking\u001b[0m/\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:38:54.926184Z","iopub.status.busy":"2024-03-14T01:38:54.925554Z","iopub.status.idle":"2024-03-14T01:38:55.859856Z","shell.execute_reply":"2024-03-14T01:38:55.858576Z","shell.execute_reply.started":"2024-03-14T01:38:54.926153Z"},"trusted":true},"outputs":[],"source":["# %mkdir working/results/\n","![ ! -d working/results/ ] && mkdir -p working/results/"]},{"cell_type":"markdown","metadata":{},"source":["# Necessary Installs and Imports"]},{"cell_type":"markdown","metadata":{},"source":["## Installs"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:39:12.684490Z","iopub.status.busy":"2024-03-14T01:39:12.683627Z","iopub.status.idle":"2024-03-14T01:39:25.710666Z","shell.execute_reply":"2024-03-14T01:39:25.709474Z","shell.execute_reply.started":"2024-03-14T01:39:12.684454Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\n","Requirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (0.7.11)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\n","Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.9.0)\n","Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2)\n","Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl) (0.7.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -U datasets transformers trl accelerate peft bitsandbytes"]},{"cell_type":"markdown","metadata":{},"source":["## HuggingFace SetUp"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from huggingface_hub import notebook_login\n","# notebook_login()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:39:41.100571Z","iopub.status.busy":"2024-03-14T01:39:41.099815Z","iopub.status.idle":"2024-03-14T01:39:42.663452Z","shell.execute_reply":"2024-03-14T01:39:42.662351Z","shell.execute_reply.started":"2024-03-14T01:39:41.100539Z"},"trusted":true},"outputs":[],"source":["!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:39:47.646552Z","iopub.status.busy":"2024-03-14T01:39:47.645574Z","iopub.status.idle":"2024-03-14T01:40:05.372224Z","shell.execute_reply":"2024-03-14T01:40:05.371417Z","shell.execute_reply.started":"2024-03-14T01:39:47.646508Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-14 01:39:56.588291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-14 01:39:56.588399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-14 01:39:56.736986: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n","from datasets import load_dataset\n","from trl import SFTTrainer\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["# Model SetUp"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:40:30.555643Z","iopub.status.busy":"2024-03-14T01:40:30.555257Z","iopub.status.idle":"2024-03-14T01:41:39.903290Z","shell.execute_reply":"2024-03-14T01:41:39.902183Z","shell.execute_reply.started":"2024-03-14T01:40:30.555615Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccf9a9e78e7d44a5b5309a477093b0cb","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0117e84e095744fe8b63f62f0a7dabd9","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4312bb2193b6402386d676892ddf80d4","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a746b7391e744ee193ad6f7485c59b8b","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a3f94593a3048cf8926f000a7b6c156","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40598d5074254bb0a79d64943bb37892","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"402b85318ef546b4bba254195a088fe0","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"meta-llama/Llama-2-7b-hf\"\n","\n","compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=compute_dtype,\n","            bnb_4bit_use_double_quant=True,\n",")\n","model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config, device_map={\"\": 0})\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:43:23.963759Z","iopub.status.busy":"2024-03-14T01:43:23.962948Z","iopub.status.idle":"2024-03-14T01:43:25.213609Z","shell.execute_reply":"2024-03-14T01:43:25.212622Z","shell.execute_reply.started":"2024-03-14T01:43:23.963725Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a64cabdb52e4d11841e7eae225a500c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d2255b810a54a3081b34284bfad2564","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e71e6e1d2cf14789a0d019a5e2dc0277","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15ce39c02b8f4474956f656a678542ee","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_eos_token=True)\n","tokenizer.pad_token = tokenizer.unk_token\n","tokenizer.padding_side = \"left\"\n","# tokenizer.add_special_tokens({'pad_token': '[PAD]'})"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:43:38.750999Z","iopub.status.busy":"2024-03-14T01:43:38.750112Z","iopub.status.idle":"2024-03-14T01:43:46.914536Z","shell.execute_reply":"2024-03-14T01:43:46.913719Z","shell.execute_reply.started":"2024-03-14T01:43:38.750967Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d6cd23e59794de49c34ba3bbe216eb4","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|██████████| 31.0M/31.0M [00:02<00:00, 11.8MB/s]\n","Downloading data: 100%|██████████| 3.86M/3.86M [00:00<00:00, 9.05MB/s]\n","Downloading data: 100%|██████████| 3.85M/3.85M [00:00<00:00, 22.4MB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f7c701ad67f4bb294e52cbfc17fd42b","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84aaffe5f4d94ad28eb76ee8824e99ea","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66798c3c46d642fbb636f42798838413","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset(\"musfiqdehan/preprocessed-BanglaNMT-sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# LoRA Configuration"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:45:26.083693Z","iopub.status.busy":"2024-03-14T01:45:26.082753Z","iopub.status.idle":"2024-03-14T01:45:26.088667Z","shell.execute_reply":"2024-03-14T01:45:26.087585Z","shell.execute_reply.started":"2024-03-14T01:45:26.083658Z"},"trusted":true},"outputs":[],"source":["peft_config = LoraConfig(\n","            lora_alpha=16, \n","            lora_dropout=0.05,\n","            r=16,\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\",\n","            target_modules= [\"down_proj\",\"up_proj\",\"gate_proj\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# peft_config = LoraConfig(\n","#             lora_alpha=16,\n","#             lora_dropout=0.05,\n","#             r=64,\n","#             bias=\"none\",\n","#             task_type=\"CAUSAL_LM\",\n","#             target_modules= [\"q_proj\",\"up_proj\",\"o_proj\",\"k_proj\",\"down_proj\",\"gate_proj\",\"v_proj\"]\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["# Training Hyperparameters"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:45:40.386143Z","iopub.status.busy":"2024-03-14T01:45:40.385269Z","iopub.status.idle":"2024-03-14T01:45:40.395195Z","shell.execute_reply":"2024-03-14T01:45:40.394413Z","shell.execute_reply.started":"2024-03-14T01:45:40.386109Z"},"trusted":true},"outputs":[],"source":["training_arguments = TrainingArguments(\n","        output_dir=\"working/results/\",\n","        evaluation_strategy=\"steps\",\n","        optim=\"paged_adamw_8bit\",\n","        save_steps=100,\n","        log_level=\"debug\",\n","        logging_steps=100,\n","        learning_rate=1e-4,\n","        eval_steps=100,\n","        fp16=True,\n","        do_eval=True,\n","        per_device_train_batch_size=48,\n","        per_device_eval_batch_size=48,\n","        gradient_accumulation_steps=2,\n","        warmup_steps=50,\n","        max_steps=500,\n","        lr_scheduler_type=\"linear\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Training with TRL"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:45:45.857270Z","iopub.status.busy":"2024-03-14T01:45:45.856339Z","iopub.status.idle":"2024-03-14T01:45:46.896707Z","shell.execute_reply":"2024-03-14T01:45:46.895448Z","shell.execute_reply.started":"2024-03-14T01:45:45.857229Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Mar 14 01:45:46 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0              32W / 250W |   5362MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:45:53.929928Z","iopub.status.busy":"2024-03-14T01:45:53.928983Z","iopub.status.idle":"2024-03-14T08:15:11.479483Z","shell.execute_reply":"2024-03-14T08:15:11.478458Z","shell.execute_reply.started":"2024-03-14T01:45:53.929890Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48e2839819f84596a7681222466ef962","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/164084 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1d584035f8a4ef99d925c6d7cd289f6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/20511 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","max_steps is given, it will override any value given in num_train_epochs\n","Using auto half precision backend\n","Currently training with a batch size of: 48\n","***** Running training *****\n","  Num examples = 164,084\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 48\n","  Total train batch size (w. parallel, distributed & accumulation) = 96\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 500\n","  Number of trainable parameters = 23,199,744\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.16.4 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/wandb/run-20240314_014637-73t3j71r</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/musfiqdehan-team/huggingface/runs/73t3j71r' target=\"_blank\">pecan-meringue-3</a></strong> to <a href='https://wandb.ai/musfiqdehan-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/musfiqdehan-team/huggingface' target=\"_blank\">https://wandb.ai/musfiqdehan-team/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/musfiqdehan-team/huggingface/runs/73t3j71r' target=\"_blank\">https://wandb.ai/musfiqdehan-team/huggingface/runs/73t3j71r</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 6:27:35, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.882500</td>\n","      <td>1.345326</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.298500</td>\n","      <td>1.265929</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.248100</td>\n","      <td>1.237133</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.229500</td>\n","      <td>1.219609</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.212900</td>\n","      <td>1.210467</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 20511\n","  Batch size = 48\n","Saving model checkpoint to working/results/tmp-checkpoint-100\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/tmp-checkpoint-100/tokenizer_config.json\n","Special tokens file saved in working/results/tmp-checkpoint-100/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","***** Running Evaluation *****\n","  Num examples = 20511\n","  Batch size = 48\n","Saving model checkpoint to working/results/tmp-checkpoint-200\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/tmp-checkpoint-200/tokenizer_config.json\n","Special tokens file saved in working/results/tmp-checkpoint-200/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","***** Running Evaluation *****\n","  Num examples = 20511\n","  Batch size = 48\n","Saving model checkpoint to working/results/tmp-checkpoint-300\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/tmp-checkpoint-300/tokenizer_config.json\n","Special tokens file saved in working/results/tmp-checkpoint-300/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","***** Running Evaluation *****\n","  Num examples = 20511\n","  Batch size = 48\n","Saving model checkpoint to working/results/tmp-checkpoint-400\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/tmp-checkpoint-400/tokenizer_config.json\n","Special tokens file saved in working/results/tmp-checkpoint-400/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","***** Running Evaluation *****\n","  Num examples = 20511\n","  Batch size = 48\n","Saving model checkpoint to working/results/tmp-checkpoint-500\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/tmp-checkpoint-500/tokenizer_config.json\n","Special tokens file saved in working/results/tmp-checkpoint-500/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=500, training_loss=1.3743009338378906, metrics={'train_runtime': 23339.8701, 'train_samples_per_second': 2.057, 'train_steps_per_second': 0.021, 'total_flos': 9.166063140864e+16, 'train_loss': 1.3743009338378906, 'epoch': 0.29})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=dataset['train'],\n","        eval_dataset=dataset['validation'],\n","        peft_config=peft_config,\n","        dataset_text_field=\"translations\",\n","        max_seq_length=48,\n","        tokenizer=tokenizer,\n","        args=training_arguments\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Inference: Translate with Llama 2"]},{"cell_type":"markdown","metadata":{},"source":["## Base Model SetUp"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T08:15:23.017181Z","iopub.status.busy":"2024-03-14T08:15:23.016952Z","iopub.status.idle":"2024-03-14T08:15:29.411201Z","shell.execute_reply":"2024-03-14T08:15:29.410050Z","shell.execute_reply.started":"2024-03-14T08:15:23.017159Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/model.safetensors.index.json\n","Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1fea45f61994f56af0e5226ef6c1c0f","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 4096,\n","  \"pad_token_id\": 0,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/tokenizer.model\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/tokenizer_config.json\n"]}],"source":["base_model = \"meta-llama/Llama-2-7b-hf\"\n","compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","        base_model, device_map={\"\": 0}, quantization_config=bnb_config\n",")\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Adapter (Fine-Tuned-Model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fetched from Kaggle Output\n","model = PeftModel.from_pretrained(model, \"working/results/checkpoint-500/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Uploaded to Hugging Face Model Hub\n","# model = PeftModel.from_pretrained(model, \"musfiqdehan/Llama-2-7b-ft-mt-Bengali-to-English-sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# Testing Manually"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T08:15:29.954389Z","iopub.status.busy":"2024-03-14T08:15:29.953990Z","iopub.status.idle":"2024-03-14T08:15:32.029173Z","shell.execute_reply":"2024-03-14T08:15:32.028014Z","shell.execute_reply.started":"2024-03-14T08:15:29.954334Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["I am going to school\n"]}],"source":["my_text = \"আমি স্কুলে যাচ্ছি ।\"\n","\n","prompt = my_text+\" ###>\"\n","\n","tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","input_ids = tokenized_input[\"input_ids\"].cuda()\n","\n","generation_output = model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n",")\n","for seq in generation_output.sequences:\n","    output = tokenizer.decode(seq, skip_special_tokens=True)\n","    print(output.split(\"###>\")[1].strip()) "]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
